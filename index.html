<!DOCTYPE html>
<html>
<head>
   <title>Voice Assistant</title>
   <style>
       body {
           background-color: #1a1a1a;
           color: white;
           font-family: Arial, sans-serif;
           display: flex;
           flex-direction: column;
           align-items: center;
           min-height: 100vh;
           margin: 0;
           padding: 20px;
       }

       .voice-button {
           width: 200px;
           height: 200px;
           border-radius: 50%;
           background: #0066ff;
           border: none;
           cursor: pointer;
           position: relative;
           transition: all 0.3s ease;
           margin: 20px;
       }

       .voice-button.recording {
           animation: pulse 1.5s infinite;
       }

       .loading {
           border: 5px solid #f3f3f3;
           border-top: 5px solid #0066ff;
           border-radius: 50%;
           width: 50px;
           height: 50px;
           animation: spin 1s linear infinite;
           position: absolute;
           top: 50%;
           left: 50%;
           transform: translate(-50%, -50%);
           display: none;
       }

       .loading.active {
           display: block;
       }

       @keyframes pulse {
           0% { transform: scale(1); }
           50% { transform: scale(1.05); }
           100% { transform: scale(1); }
       }

       @keyframes spin {
           0% { transform: translate(-50%, -50%) rotate(0deg); }
           100% { transform: translate(-50%, -50%) rotate(360deg); }
       }

       #response {
           margin: 20px;
           padding: 20px;
           background: rgba(255, 255, 255, 0.1);
           border-radius: 10px;
           max-width: 600px;
           width: 100%;
       }

       #recordings {
           display: none;
       }

       .status-text {
           text-align: center;
           margin-top: 20px;
           font-size: 14px;
           color: #888;
       }
   </style>
</head>
<body>
   <div class="voice-button" id="talkButton">
       <div class="loading" id="loadingIndicator"></div>
   </div>
   <div class="status-text">Standard voice</div>
   <div id="response"></div>
   <div id="recordings"></div>
   
   <script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script>
   <script src="config.js"></script>
   <script>
       const ASSISTANT_ID = 'asst_CDvFyzLG7KL7aRuTymxyrY9W';
       const button = document.querySelector('.voice-button');
       const responseDiv = document.getElementById('response');
       const recordingsDiv = document.getElementById('recordings');
       const loadingIndicator = document.getElementById('loadingIndicator');
       let mediaRecorder;
       let audioChunks = [];

       async function sendToOpenAI(blob) {
           try {
               loadingIndicator.classList.add('active'); // Show loading
               const formData = new FormData();
               formData.append('file', blob, 'audio.webm');
               formData.append('model', 'whisper-1');

               const transcriptionResponse = await axios.post('https://api.openai.com/v1/audio/transcriptions', formData, {
                   headers: {
                       'Authorization': `Bearer ${window.OPENAI_API_KEY}`,
                       'Content-Type': 'multipart/form-data'
                   }
               });

               const userText = transcriptionResponse.data.text;

               const threadResponse = await axios.post('https://api.openai.com/v1/threads', {}, {
                   headers: {
                       'Authorization': `Bearer ${window.OPENAI_API_KEY}`,
                       'OpenAI-Beta': 'assistants=v2'
                   }
               });

               const messageResponse = await axios.post(`https://api.openai.com/v1/threads/${threadResponse.data.id}/messages`, {
                   role: 'user',
                   content: userText
               }, {
                   headers: {
                       'Authorization': `Bearer ${window.OPENAI_API_KEY}`,
                       'OpenAI-Beta': 'assistants=v2'
                   }
               });

               const runResponse = await axios.post(`https://api.openai.com/v1/threads/${threadResponse.data.id}/runs`, {
                   assistant_id: ASSISTANT_ID
               }, {
                   headers: {
                       'Authorization': `Bearer ${window.OPENAI_API_KEY}`,
                       'OpenAI-Beta': 'assistants=v2'
                   }
               });

               let runStatus;
               do {
                   await new Promise(resolve => setTimeout(resolve, 1000));
                   const statusResponse = await axios.get(`https://api.openai.com/v1/threads/${threadResponse.data.id}/runs/${runResponse.data.id}`, {
                       headers: {
                           'Authorization': `Bearer ${window.OPENAI_API_KEY}`,
                           'OpenAI-Beta': 'assistants=v2'
                       }
                   });
                   runStatus = statusResponse.data.status;
               } while (runStatus === 'in_progress');

               const messagesResponse = await axios.get(`https://api.openai.com/v1/threads/${threadResponse.data.id}/messages`, {
                   headers: {
                       'Authorization': `Bearer ${window.OPENAI_API_KEY}`,
                       'OpenAI-Beta': 'assistants=v2'
                   }
               });

               const aiResponse = messagesResponse.data.data[0].content[0].text.value;

               const speechResponse = await axios.post('https://api.openai.com/v1/audio/speech', {
                   model: "tts-1",
                   input: aiResponse,
                   voice: "alloy"
               }, {
                   headers: {
                       'Authorization': `Bearer ${window.OPENAI_API_KEY}`,
                       'Content-Type': 'application/json'
                   },
                   responseType: 'arraybuffer'
               });

               const audioBlob = new Blob([speechResponse.data], { type: 'audio/mpeg' });
               const audioUrl = URL.createObjectURL(audioBlob);
               const audio = new Audio(audioUrl);
               audio.play();

               responseDiv.innerHTML = `You said: ${userText}<br><br>AI response: ${aiResponse}`;

           } catch (error) {
               console.log('Full error:', error.response?.data);
               if (error.response?.data?.error) {
                   console.log('Error message:', error.response.data.error.message);
               }
               responseDiv.textContent = 'Error processing audio';
           } finally {
               loadingIndicator.classList.remove('active'); // Hide loading
           }
       }

       button.onclick = async () => {
           if (!mediaRecorder) {
               const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
               mediaRecorder = new MediaRecorder(stream, {
                   mimeType: 'audio/webm;codecs=opus'
               });
               
               mediaRecorder.ondataavailable = (event) => {
                   audioChunks.push(event.data);
               };

               mediaRecorder.onstop = async () => {
                   const audioBlob = new Blob(audioChunks, {
                       type: 'audio/webm'
                   });
                   await sendToOpenAI(audioBlob);
                   
                   button.classList.remove('recording');
                   audioChunks = [];
               };

               button.classList.add('recording');
               mediaRecorder.start();
           } else {
               mediaRecorder.stop();
               mediaRecorder = null;
           }
       };
   </script>
</body>
</html>