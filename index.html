<!DOCTYPE html>
<html>
<head>
    <title>Voice Assistant</title>
    <style>
         body {
              background-color: #1a1a1a;
              color: white;
              font-family: Arial, sans-serif;
              display: flex;
              flex-direction: column;
              align-items: center;
              min-height: 100vh;
              margin: 0;
              padding: 20px;
         }

         .voice-button {
              width: 200px;
              height: 200px;
              border-radius: 50%;
              background: #0066ff;
              border: none;
              cursor: pointer;
              position: relative;
              transition: all 0.3s ease;
              margin: 20px;
         }

         .voice-button.recording {
              animation: pulse 1.5s infinite;
         }

         .loading {
              border: 5px solid #f3f3f3;
              border-top: 5px solid #0066ff;
              border-radius: 50%;
              width: 50px;
              height: 50px;
              animation: spin 1s linear infinite;
              position: absolute;
              top: 50%;
              left: 50%;
              transform: translate(-50%, -50%);
              display: none;
         }

         .loading.active {
              display: block;
         }

         @keyframes pulse {
              0% { transform: scale(1); }
              50% { transform: scale(1.05); }
              100% { transform: scale(1); }
         }

         @keyframes spin {
              0% { transform: translate(-50%, -50%) rotate(0deg); }
              100% { transform: translate(-50%, -50%) rotate(360deg); }
         }

         #response {
              margin: 20px;
              padding: 20px;
              background: rgba(255, 255, 255, 0.1);
              border-radius: 10px;
              max-width: 600px;
              width: 100%;
         }

         #recordings {
              display: none;
         }

         .status-text {
              text-align: center;
              margin-top: 20px;
              font-size: 14px;
              color: #888;
         }
    </style>
</head>
<body>
    <div class="voice-button" id="talkButton">
        <div class="loading" id="loadingIndicator"></div>
    </div>
    <div class="status-text">Standard voice</div>
    <div id="response"></div>
    <div id="recordings"></div>
    
    <script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script>
    <script src="config.js"></script>
    <script>
        const ASSISTANT_ID = 'asst_CDvFyzLG7KL7aRuTymxyrY9W';
        const button = document.querySelector('.voice-button');
        const responseDiv = document.getElementById('response');
        const loadingIndicator = document.getElementById('loadingIndicator');
        let mediaRecorder;
        let audioChunks = [];

        async function sendToOpenAI(blob) {
            try {
                loadingIndicator.classList.add('active'); // Show loading

                // Step 1: Transcribe audio
                const formData = new FormData();
                formData.append('file', blob, 'audio.webm');
                formData.append('model', 'whisper-1');

                const transcriptionResponse = await axios.post('/api/openai/audio/transcriptions', formData, {
                    headers: {
                        'Content-Type': 'multipart/form-data'
                    }
                });
                const userText = transcriptionResponse.data.text;

                // Step 2: Create a thread
                const threadResponse = await axios.post('/api/openai/threads');

                // Step 3: Send user message to the thread
                await axios.post(`/api/openai/threads/${threadResponse.data.id}/messages`, {
                    role: 'user',
                    content: userText
                });

                // Step 4: Run the assistant
                const runResponse = await axios.post(`/api/openai/threads/${threadResponse.data.id}/runs`, {
                    assistant_id: ASSISTANT_ID
                });

                // Step 5: Check run status
                let runStatus;
                do {
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    const statusResponse = await axios.get(`/api/openai/threads/${threadResponse.data.id}/runs/${runResponse.data.id}`);
                    runStatus = statusResponse.data.status;
                } while (runStatus === 'in_progress');

                // Step 6: Get the assistant's response
                const messagesResponse = await axios.get(`/api/openai/threads/${threadResponse.data.id}/messages`);
                const aiResponse = messagesResponse.data.data[0].content[0].text.value;

                // Step 7: Convert text to speech
                const speechResponse = await axios.post('/api/openai/audio/speech', {
                    model: "tts-1",
                    input: aiResponse,
                    voice: "alloy"
                }, {
                    responseType: 'arraybuffer'
                });

                // Step 8: Play the audio
                const audioBlob = new Blob([speechResponse.data], { type: 'audio/mpeg' });
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                audio.play();

                // Step 9: Display the response
                responseDiv.innerHTML = `You said: ${userText}<br><br>AI response: ${aiResponse}`;

            } catch (error) {
                console.error('Error:', error.response?.data || error.message);
                responseDiv.textContent = 'Error processing audio';
            } finally {
                loadingIndicator.classList.remove('active'); // Hide loading
            }
        }

        button.onclick = async () => {
            if (!mediaRecorder) {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'audio/webm;codecs=opus'
                });
                
                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    await sendToOpenAI(audioBlob);
                    button.classList.remove('recording');
                    audioChunks = [];
                };

                button.classList.add('recording');
                mediaRecorder.start();
            } else {
                mediaRecorder.stop();
                mediaRecorder = null;
            }
        };
    </script>
</body>
</html>
